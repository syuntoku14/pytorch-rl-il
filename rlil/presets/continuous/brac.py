import torch
from torch import nn
from torch.optim import Adam
from rlil.agents import BRAC
from rlil.approximation import (QContinuous,
                                PolyakTarget,
                                BcqEncoder,
                                BcqDecoder)
from rlil.policies import SoftDeterministicPolicy
from rlil.memory import ExperienceReplayBuffer
from rlil.initializer import (get_device,
                              set_replay_buffer,
                              disable_on_policy_mode)
from .models import (fc_q,
                     fc_soft_policy,
                     fc_bcq_encoder,
                     fc_bcq_decoder)


def brac(
        transitions=None,
        # Common settings
        discount_factor=0.99,
        # Adam optimizer settings
        lr_q=1e-3,
        lr_pi=1e-3,
        # Training settings
        bc_iters=5000,
        minibatch_size=100,
        polyak_rate=0.005,
        alpha=0.1
):
    """
    Bootstrapping error accumulation reduction (BEAR) control preset

    Args:
        transitions:
            dictionary of transitions generated by cpprb.ReplayBuffer.get_all_transitions() 
        discount_factor (float): Discount factor for future rewards.
        lr_q (float): Learning rate for the Q network.
        lr_pi (float): Learning rate for the policy network.
        alpha (float): Value of lagrange multipliers. Trick 3.
        minibatch_size (int): Number of experiences to sample in each training update.
        polyak_rate (float): Speed with which to update the target network towards the online network.
    """
    def _brac(env):
        disable_on_policy_mode()

        device = get_device()
        q_1_model = fc_q(env).to(device)
        q_1_optimizer = Adam(q_1_model.parameters(), lr=lr_q)
        q_1 = QContinuous(
            q_1_model,
            q_1_optimizer,
            target=PolyakTarget(polyak_rate),
            name='q_1'
        )

        q_2_model = fc_q(env).to(device)
        q_2_optimizer = Adam(q_2_model.parameters(), lr=lr_q)
        q_2 = QContinuous(
            q_2_model,
            q_2_optimizer,
            target=PolyakTarget(polyak_rate),
            name='q_2'
        )

        policy_model = fc_soft_policy(env).to(device)
        policy_optimizer = Adam(policy_model.parameters(), lr=lr_pi)
        policy = SoftDeterministicPolicy(
            policy_model,
            policy_optimizer,
            env.action_space,
            target=PolyakTarget(polyak_rate),
        )

        behavior_model = fc_soft_policy(env).to(device)
        behavior_optimizer = Adam(behavior_model.parameters(), lr=lr_pi)
        behavior_policy = SoftDeterministicPolicy(
            behavior_model,
            behavior_optimizer,
            env.action_space,
            target=PolyakTarget(polyak_rate),
            name='behavior_policy'
        )

        replay_buffer = ExperienceReplayBuffer(1e7, env)
        if transitions is not None:
            samples = replay_buffer.samples_from_cpprb(
                transitions, device="cpu")
            replay_buffer.store(samples)
        set_replay_buffer(replay_buffer)

        return BRAC(
            q_1=q_1,
            q_2=q_2,
            policy=policy,
            behavior_policy=behavior_policy,
            bc_iters=bc_iters,
            alpha=alpha,
            discount_factor=discount_factor,
            minibatch_size=minibatch_size,
        )
    return _brac


__all__ = ["brac"]
